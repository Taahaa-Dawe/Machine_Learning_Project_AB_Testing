# -*- coding: utf-8 -*-
"""SVM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfCLR7ATb3d6DosI3y99IbwJK8ahWrjv

To show how a 2D point is transformed using a polynomial kernel with \( r = 1 \) and \( d = 2 \), we'll start with a simple 2D point and apply the transformation. The polynomial kernel transformation is given by the formula:

$$
\phi(\mathbf{x}) = (1 + \mathbf{x}^T \mathbf{x})^d
$$

Where:
$( \mathbf{x})$ is the original vector.
$( d )$ is the degree of the polynomial (in this case, 2).
$( r )$ is the coefficient that scales the input features before applying the polynomial expansion (in this case, 1).

For a 2D point $( \mathbf{x} = (x_1, x_2) )$, this formula expands to:

$$
\phi(\mathbf{x}) = (1 + x_1^2 + x_2^2 + 2x_1x_2)^2
$$

Expanding this polynomial, you get:

$$
\phi(\mathbf{x}) = 1 + x_1^4 + x_2^4 + 4x_1^3x_2 + 4x_1x_2^3 + 6x_1^2x_2^2 + 4x_1^2 + 4x_2^2 + 4x_1x_2
$$

This polynomial includes terms up to the fourth degree due to the interaction between $( x_1 )$ and $( x_2 )$. When this point is "cast" into a higher-dimensional space, each unique term in the expansion corresponds to a dimension in that space. Thus, the transformed vector $( \phi(\mathbf{x}))$ would be represented in the higher-dimensional space as:

$$
\phi(\mathbf{x}) = (1, x_1^4, x_2^4, 4x_1^3x_2, 4x_1x_2^3, 6x_1^2x_2^2, 4x_1^2, 4x_2^2, 4x_1x_2)
$$
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix , classification_report, ConfusionMatrixDisplay
import numpy as np

Data = pd.read_csv("/content/Wholedata.csv")
Data.dropna(inplace=True)
Data = Data.drop(["Date"], axis=1)
TrainDF, TestDF = train_test_split(Data, test_size=0.30, random_state= 31)

TrainDF.head()

TrainLabels = TrainDF["Campaign Name"]

TestLabels = TestDF["Campaign Name"]

TrainDF.drop("Campaign Name", axis =1, inplace= True)

TestDF.drop("Campaign Name", axis =1, inplace= True)

from sklearn.svm import LinearSVC
import seaborn as sns

SVM_Linear1 =LinearSVC(C=10)
SVM_Linear1.fit(TrainDF,TrainLabels)
y_pred = SVM_Linear1.predict(TestDF)

cnf_matrix1 = confusion_matrix(TestLabels, y_pred, labels = SVM_Linear1.classes_)

disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix1,
                               display_labels=SVM_Linear1.classes_)
disp.plot()

print(classification_report(TestLabels, y_pred))

SVM_Linear40 =LinearSVC(C=40)
SVM_Linear40.fit(TrainDF,TrainLabels)
y_pred = SVM_Linear40.predict(TestDF)

cnf_matrix1 = confusion_matrix(TestLabels, y_pred, labels = SVM_Linear1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix1,
                               display_labels=SVM_Linear1.classes_)
disp.plot()

print(classification_report(TestLabels, y_pred))

from sklearn.svm import SVC
SVM_rbf = SVC(C = 1,kernel='rbf')
SVM_rbf.fit(TrainDF,TrainLabels)
y_pred = SVM_rbf.predict(TestDF)

cnf_matrix1 = confusion_matrix(TestLabels, y_pred, labels = SVM_Linear1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix1,
                               display_labels=SVM_rbf.classes_)
disp.plot()

print(classification_report(TestLabels, y_pred))

from sklearn.svm import SVC
SVM_ploy = SVC(C = 40,kernel='poly',degree = 3)
SVM_ploy.fit(TrainDF,TrainLabels)
y_pred = SVM_ploy.predict(TestDF)

cnf_matrix1 = confusion_matrix(TestLabels, y_pred, labels = SVM_Linear1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix1,
                               display_labels=SVM_rbf.classes_)
disp.plot()

print(classification_report(TestLabels, y_pred))

from sklearn.svm import SVC
SVM_ploy = SVC(C = 2,kernel='poly',degree = 3)
SVM_ploy.fit(TrainDF,TrainLabels)
y_pred = SVM_ploy.predict(TestDF)

cnf_matrix1 = confusion_matrix(TestLabels, y_pred, labels = SVM_Linear1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix1,
                               display_labels=SVM_rbf.classes_)
disp.plot()

print(classification_report(TestLabels, y_pred))

import matplotlib.pyplot as plt

# Define the kernel types and their corresponding accuracies based on the data provided in the chat.
kernels = ['Linear (C=10)', 'Linear (C=40)', 'RBF (C=1)', 'Poly (deg=3, C=40)', 'Poly (deg=3, C=2)']
accuracies = [0.78, 0.89, 0.61, 0.61, 0.61]  # Accuracy for each kernel

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(kernels, accuracies, marker='o', linestyle='-', color='b')

# Title and labels
plt.title('Comparison of Kernel Accuracies', fontsize=16)
plt.xlabel('Kernel and Cost Parameter', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.xticks(rotation=45)
plt.grid(True)

# Display the plot
plt.tight_layout()
plt.show()